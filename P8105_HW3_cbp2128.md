Homework 3 Solutions
================
Christian Pascual
10/4/2018

Preliminary imports and option settings
=======================================

``` r
library(tidyverse)
library(knitr)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.align = "center",
  out.width = "90%"
)

theme_set(theme_minimal())
```

This homework focuses on visualization in exploratory data analysis, so we need the plotting from `tidyverse` and the data from `p8105.datasets`. `patchwork` is for placing figures nicely side-by-side, and `knitr` is for the nice tables. I like the minimal theme, so I've set it for this homework.

I've laid out figure presets from class and some from *R for Data Science*.

Problem 1: BRFSS
================

Data loading and cleaning
-------------------------

This problem requires the BRFSS dataset, which we'll pull from `p8105.datasets`. We'll clean the dataset a bit before diving into answering the questions.

``` r
raw_BRFSS = p8105.datasets::brfss_smart2010

tidy_BRFSS = raw_BRFSS %>% 
  janitor::clean_names() %>% 
  filter(., topic == "Overall Health") %>% 
  mutate(., response = as.factor(response))
```

We've limited our BRFSS dataset to the "Overall Health" topic and converted the `response` column into a factor variable from a string. We can now start asking questions from the data.

Problem 1 questions
-------------------

#### In 2002, which states were observed at 7 locations?

The `locationdesc` column describes where the survey was conducted in a state, and the year encodes when the survey was administered. Thus, we need to use these columns to answer this question. Each state is associated with multiple locations in `locationdesc`, so we can group the states and then count the unique values in this column.

``` r
states_w_7_locs_in_2002 = tidy_BRFSS %>% 
  filter(., year == 2002) %>% 
  group_by(locationabbr) %>% 
  summarize(num_rows = n(),
            num_unique_locs = n_distinct(locationdesc)) %>% 
  filter(., num_unique_locs == 7)

kable(states_w_7_locs_in_2002)
```

| locationabbr |  num\_rows|  num\_unique\_locs|
|:-------------|----------:|------------------:|
| CT           |         35|                  7|
| FL           |         35|                  7|
| NC           |         35|                  7|

The result is the tibble above. There were 3 states (Conneticut, Florida, and North Carolina) with 7 locations listed in 2002. This result suggests that only a few states had such extensive surveying done in the early years of the dataset.

#### Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

A spaghetti plot seems to be a regular line plot with many, many lines on it. For the BRFSS dataset, our `x` axis will take on the years from 2002 to 2010, and the `y` axis will plot the amount of observations for each state. Each line will correspond to a state, leaving us with 51 lines for each state with one for the District of Columbia.

``` r
spaghetti_states = tidy_BRFSS %>% 
  filter(., year >= 2002 & year <= 2010) %>% 
  group_by(locationabbr, year) %>% 
  summarize(num_rows = n(),
            n_locs = length(unique(locationdesc)))

ggplot(data = spaghetti_states, 
       aes(x = year, y = n_locs, color = locationabbr)) +
  geom_line(alpha = 0.5) + 
  labs(
    title = "Spaghetti plot of number of unique locations by state from 2002 - 2010",
    x = "Year",
    y = "Number of unique locations"
  ) + 
  theme(legend.position = "none")
```

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/P1_spaghetti_states-1.png" width="90%" style="display: block; margin: auto;" />

Because the plot contains information on all 50 states, a lot of information is compacted and missed. There is a lot of clumping at the bottom of the graph since many states with relatively small amounts of observations. I've hidden the legend since it compacts the plot and doesn't add a lot of information anyways. This graph suggests that most of the states only have a few locations that are surveyed. The lines that rise above the rest are large states that Florida and Texas, indicating that they are heavily represented in the survey. Most of the states have less than 10 locations.

#### Make a table for the years 2002, 2006, and 2010 and show the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

To make this table, we need to filter out all the years except for 2002, 2006, and 2010. Then, we can use `group_by()` to gather everything and use `summarize()` to get the mean and standard deviation.

``` r
relevant_years = c(2002, 2006, 2010)

tabulated_stats = tidy_BRFSS %>% 
  filter(., year %in% relevant_years & 
           response == "Excellent" & 
           locationabbr == "NY") %>% 
  group_by(year) %>% 
  summarize(`Mean Excellent prop.` = round(mean(data_value), 1),
            `Std. of Excellent prop.` = round(var(data_value), 1))

kable(tabulated_stats)
```

|  year|  Mean Excellent prop.|  Std. of Excellent prop.|
|-----:|---------------------:|------------------------:|
|  2002|                  24.0|                     20.1|
|  2006|                  22.5|                     16.0|
|  2010|                  22.7|                     12.7|

According to `tabulated_stats`, the mean proportion of "Excellent scores" remains relatively constant from 2002 to 2010, but the variance decreases in this same time frame. This trend suggests that the various locations in NY surveyed have started to agree on the degree of "excellence".

#### For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.

This is a graph that requires 5 separate charts that look at proportion for each `response` value through time. Thus, we'll need to `group_by()` both state and year, and then `summarize()` the `data_value` for each `response` by mean.

``` r
avgd_resp_thru_time = tidy_BRFSS %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(avg_prop = mean(data_value))

ggplot(avgd_resp_thru_time, aes(x = year, y = avg_prop)) +
  geom_line(aes(color = locationabbr), alpha = 0.5) +
  facet_grid(response ~ .) +
  theme(panel.spacing = unit(1, "lines"), legend.position = "none") + 
  labs(
    title = "Average response proportion by state from 2002 - 2010",
    x = "Year",
    y = "Average Response Proportion"
  )
```

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/P1_five_panel_bonanza-1.png" width="90%" style="display: block; margin: auto;" />

These sets of plots demonstrate distinct differences between the average proportion in each sate through time. Each individual plot has 50 lines on it, so it is infeasible to look at trends in single states. In each plot, we can see that the spread of response proportions in all states remains in a relatively stable band through time. For example, the "Poor" average responses in the 50 states rarely exceed 10%, while the "Very good" responses vary from 25 - 45%.

Problem 2: Instacart
====================

This problem focuses on exploring the Instacart dataset, contained in `p8105.datasets`. To get started, we'll grab the data.

``` r
instacart = p8105.datasets::instacart
```

A cursory look at the Instacart data set
----------------------------------------

The `instacart` dataset has 1384617 and 15 columns, making it exceptionally large. In general, the dataset describes the orders and reorders of Instacart users in 2017. We can look at the contents of particular orders by `order_id` and can look at the `department` and `aisle` a product comes from. Along with `product_id`s, we also have access to the precise name of the product in `product_name`, which range from "Bulgarian Yogurt" to "Sustainably Soft Bath Tissue".

The most interesting aspect of the data is perhaps the extensive order information, which we can link not just to users by `user_id` but also if the item is `reordered` or not. The resolution of the data is down to the hour, making it exceptionally granular. We can look at how many orders a user has done (`order_number`) and the precise *order* that a customer added an item to their cart. We may not have all of a user's orders in the dataset, but we do know if an item was `reordered` or not. For example, `user_id 1` ordered 11 items in their 11th order, and 10 of these items were reorders. This data suggests that this user is a frequent user of the site and uses the site to get their regular groceries, based on the items ordered (ie, Whole Milk, Paper Towels and Cinnamon Toast Crunch). Since this seems to be training data, this dataset can no doubt be used to segment Instacart's customers in a machine learning process.

Problem 2 questions
-------------------

#### How many aisles are there, and which aisles are the most items ordered from?

This question requires us to group the aisles and count the number of rows present for each grouping.

``` r
aisle_grouping = instacart %>% 
  group_by(aisle, aisle_id) %>% 
  summarize(n_items = n()) %>% 
  arrange(-n_items)
```

`aisle_grouping` tells us that there are 134 aisles. By counting the number of times an `aisle_id` appears, we can use this to find out which aisles have most items. Each row represents an order, so we would have the aisles with the most items ordered.

The top 3 aisles that are ordered from are "fresh vegetables", "fresh fruits", and "packaged vegetable fruits". This finding makes sense, since these items spoil fast and thus require frequent reordering. Or perhaps Instacart users are particularly health-inclined.

#### Make a plot that shows the number of items ordered in each aisle.

With `aisle_grouping`, we can create a plot of the ordered items by aisle. There are 134 aisles, so the coordinates should be flipped to accomodate the aisle names. We have the number of items ordered for each aisle in `n_items`, so we'll plot those as points to keep the descending order.

``` r
ggplot(data = aisle_grouping, 
       aes(x = reorder(aisle, n_items), y = n_items, color = aisle_id)) +
  geom_point() + 
  coord_flip() + 
  labs(
    title = "Number of items ordered per aisle in Instacart dataset",
    x = "Aisle description",
    y = "Number of items ordered"
  ) + 
  theme(legend.position = "none")
```

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/P2_items_by_aisles-1.png" width="90%" style="display: block; margin: auto;" />

The distribution of items ordered seems to follow a power law distribution, given the sharp fall off after the top 4 aisles. From there, we see that the number of items ordererd slowly descreases to a near-asymptote. I am also honestly surprised by how much sparkling water is ordered.

#### Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

To approach this problem, we require a grouping of the `instacart` data on both the `aisle` and the `product_name`. We can then `summarize()` based on the count of the rows in each of these groups and pick out the exact aisles we need for the table after filtering by rank.

The resulting table has the most popular item in the desired aisles.

``` r
relevant_aisles = c("baking ingredients", "dog food care", "packaged vegetables fruits")

most_popular_items = instacart %>% 
  group_by(aisle, product_name) %>% 
  summarize(times_ordered = n()) %>% 
  filter(min_rank(desc(times_ordered)) < 2,
         aisle %in% relevant_aisles) %>% 
  arrange(-times_ordered)

kable(most_popular_items)
```

| aisle                      | product\_name                                 |  times\_ordered|
|:---------------------------|:----------------------------------------------|---------------:|
| packaged vegetables fruits | Organic Baby Spinach                          |            9784|
| baking ingredients         | Light Brown Sugar                             |             499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |              30|

#### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

This problem requires a grouping and subsequent spreading of the summary data. We need to group by the relevant items and day of the week, summarizing on the `order_hour_of_day`. After, we spread these rows out into columns for reading ease.

``` r
proper_colnames = c("product_name", "Sunday", "Monday", "Tuesday",
                    "Wednesday", "Thursday", "Friday", "Saturday")
pink_and_coffee = instacart %>% 
  filter(., product_name == "Pink Lady Apples" | 
           product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_order_hour = round(mean(order_hour_of_day), 2)) %>% 
  spread(key = order_dow, value = mean_order_hour)

colnames(pink_and_coffee) = proper_colnames

kable(pink_and_coffee)
```

| product\_name    |  Sunday|  Monday|  Tuesday|  Wednesday|  Thursday|  Friday|  Saturday|
|:-----------------|-------:|-------:|--------:|----------:|---------:|-------:|---------:|
| Coffee Ice Cream |   13.77|   14.32|    15.38|      15.32|     15.22|   12.26|     13.83|
| Pink Lady Apples |   13.44|   11.36|    11.70|      14.25|     11.55|   12.78|     11.94|

The table tells us that, on average, coffee ice cream is ordered later in the day than Pink Lady Apples. The greatest difference in mean purchase hour is about 4 on Tuesdays, indicating customer realizations that it is only the middle of the week still and they have only 2 days until Data Science homework is due.

Problem 3: NY NOAA
==================

This problem focuses on exploration of the `NY NOAA` dataset. We'll bring in the data first before anything else.

``` r
raw_NY_NOAA = p8105.datasets::ny_noaa
```

A cursory look at the NT NOAA data set
--------------------------------------

In its raw form, the NOAA data set is quite large, coming in at 2595176 rows and 7 columns. The key variables in the dataset describe precipitation (`prcp`), snowfall (`snow`) and temperature (`tmax` and `tmin`) from 01-01-1981 to 12-31-2010. Missing values are *rampant* in the dataset, since we are told that some weather stations collect subsets of the variables. Some rows have missing data for all the weather variables, indicating that neither rain nor snow fell, and temperature was not recorded at that station. With such a large extent of the data missing, we must be midndful of how we filter and look at the data.

Cleaning the NOAA dataset
-------------------------

We'd like to separate the date into its constituent parts and standardize the units between `prcp`, `snow` and `snwd`. The temperature columns need to be converted into doubles (tricky) brought into degrees, rather than tenths of a degree as well.

``` r
clean_NY_NOAA = raw_NY_NOAA %>% 
  separate(., date, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(month = as.integer(month),
         day = as.integer(day),
         prcp_in = ifelse(!is.na(prcp), prcp / 10, NA),
         tmax_C = ifelse(!is.na(tmax), as.double(tmax) / 10, NA),
         tmin_C = ifelse(!is.na(tmin), as.double(tmin) / 10, NA))
```

Problem 3 Questions
-------------------

#### For snowfall, what are the most commonly observed values? Why?

We can get a visualization of the frequency of snowfall measurements in a histogram

``` r
ggplot(data = clean_NY_NOAA,
       aes(snow)) +
  geom_histogram()
```

    ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.

    ## Warning: Removed 381221 rows containing non-finite values (stat_bin).

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/P3_wheres_my_snow-1.png" width="90%" style="display: block; margin: auto;" />

New York's temperautres range greatly throughout the year, and snowfall can only happen when it's below 0°C. It only gets that cold in the winter season, so we can expect that we New York won't get any snowfall when it's warmer. Thus, the most frequently observed snow depth in the dataset is 0.

#### Make a two-panel plot showing the average temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?

This questions wants to look at January and July through time on the scale of years. We need two plots since question specifies two-panel, so `patchwork` will do that work for us. We can only calculate average temperature when we have both `tmax` and `tmin`, so we have to also filter out rows which do not have both of these values.

``` r
jan_jul_temps = clean_NY_NOAA %>% 
  filter(., month == 1 | month == 7,
         !is.na(tmax_C) & !is.na(tmin_C)) %>% 
  mutate(sample_temp_avg = (tmax_C + tmin_C) / 2)

jan_temps = ggplot(data = filter(jan_jul_temps, month == 1),
                   aes(x = year, y = sample_temp_avg)) +
  geom_boxplot()
jul_temps = ggplot(data = filter(jan_jul_temps, month == 7),
                   aes(x = year, y = sample_temp_avg)) +
  geom_boxplot()

jan_temps / jul_temps
```

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/P3_jan_vs_jul_temps-1.png" width="90%" style="display: block; margin: auto;" />

Since each station measures a different maximum and minimum temperature, my approach was to look at the distribution of these station measurements with a boxplot. We see that the average temperature in January is much lower than July through the duration of 1981 to 2010, hovering below 0°C. Both January and July have outlier measurements through time, but January seems to have greater variance.

#### Make a two-panel plot showing 1) tmax vs tmin for the full dataset and 2) a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

These two plots require us to look at the entire dataset, so no filters here. We will have one plot for `tmax_C` and `tmin_C` and another for `snow_in` distribution through time.

``` r
min_max_pairs = ggplot(data = clean_NY_NOAA, 
                   aes(x = tmax_C, y = tmin_C)) +
  geom_bin2d(bins = c(50, 50))

snowfall_thru_time = clean_NY_NOAA %>% 
  filter(., snow > 0 & snow < 100) %>% 
  group_by(year) %>% 
  summarize(n_rows = n(),
            avg_snow = mean(snow),
            std_err_snow = (var(snow) / n_rows)**(1/2)) %>% 
  ggplot(data = ., aes(x = year, group = 1)) +
  geom_line(aes(y = avg_snow, fill = "blue")) +
  geom_ribbon(aes(ymin = avg_snow - 1.96 * std_err_snow, 
                  ymax = avg_snow + 1.96 * std_err_snow), 
              fill = "blue", alpha = 0.5)
```

    ## Warning: Ignoring unknown aesthetics: fill

``` r
min_max_pairs
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_bin2d).

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/temp_thru_time-1.png" width="90%" style="display: block; margin: auto;" />

``` r
snowfall_thru_time
```

<img src="P8105_HW3_cbp2128_files/figure-markdown_github/temp_thru_time-2.png" width="90%" style="display: block; margin: auto;" />

The `tmax_C` vs `tmin_C` comparison gives us an elliptical shape that suggests that the ranges for the two variables are different. Most of the heat ranges from 0°C to 30°C for `tmax_C` and from 0°C to 15°C for `tmin_C`. These ranges capture the lows of winter to the balmy summers.

The distribution of `snow` shows that snowfall remained relatively stable around 32mm of snow up until 2005. The years after that suggest a drop in the amount of snowfall. The solid line describes the trend in mean snowfall amount, and the ribbon captures a 95% confidence interval for the mean snowfall for each year. The distirbution of snowfall values between 0 and 100 are slightly right-skewed, but the sheer amount of data points can reduce the sampling variance enough to be reasonably normal.
